---
title: "Final Project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Loading data}
# % of food intake (kg) in countries around the world
food_supply_o <- read.csv("Food_Supply_Quantity_kg_Data.csv")
# % of energy intake (kcal) from different types of food in countries around the world. 
kcal_intake_o <- read.csv("Food_Supply_kcal_Data.csv")
# % of fat intake from different types of food in countries around the world
fat_intake_o <- read.csv("Fat_Supply_Quantity_Data.csv")
# % of protein intake from different types of food in countries around the world
protein_intake_o <- read.csv("Protein_Supply_Quantity_Data.csv")

# Select only required columns
#country <- food_supply_o[, 1]
food_supply <- food_supply_o[, c(1:31)]
kcal_intake <- kcal_intake_o[, c(2:24)]
fat_intake <- fat_intake_o[, c(2:24)]
protein_intake <- protein_intake_o[, c(2:24)]
```

```{r Renaming columns }
#defining the function to output new columns names 
rename_columns <- function(prefix, dataframe){
  new_names <- c()
  for (i in colnames(dataframe)){
    i = paste(prefix, i, sep = "")
    new_names <- c(new_names, i)
  }
  return(new_names)
}

#renaming column names of the dataframes
colnames(kcal_intake) <- rename_columns("kcal_",kcal_intake)
colnames(fat_intake) <- rename_columns("fat_",fat_intake)
colnames(protein_intake) <- rename_columns("protein_",protein_intake)
```

```{r Creating the final dataframe}
# cbind function to combine dfs
combined <- cbind(food_supply, kcal_intake, fat_intake, protein_intake)
names(combined)
ncol(combined)

#creating index
rownames(combined) <- food_supply_o[, 1]
head(combined)
```

```{r Transforming Undernourished Variable}
combined$Undernourished=ifelse(combined$Undernourished=="<2.5",1.5, combined$Undernourished)
#convert char to numeric
combined$Undernourished = as.numeric(combined$Undernourished)
# Treat Undernourished - Replace all '<2.5' with 1.5. And divide into 3 bins by value
combined$Undernourished <- cut(combined$Undernourished, breaks = 3, labels = c("Low", "Mid", "High"))
```


```{r Processing null values}
#dropping active, confirmed, recovered
drop_list <- c('Active', 'Recovered', 'Confirmed')
combined <- combined[ , !names(combined) %in% drop_list]

#dropping null values from target variable 
library(tidyr)
combined <- combined %>% drop_na('Deaths')

#creating a dataframe for count of nulls in each column in the combined dataframe 
nulls <- colSums(is.na(combined))
na_count <- data.frame( nulls)

library(dplyr) 
na_count %>% filter(nulls > 0)

#data with labels
full_data <- combined

#data without labels for modelling
combined <- subset(combined,select = -c(Country))
```

```{r Correlation}
##Exclude Undernourished and Country names column for numerical analyses
combined_numeric <- subset(combined,select = -c(Undernourished))

# Create correlation matrix
cor_matrix <- cor(combined_numeric, method = c("pearson"))
```

```{r Drop correlated variables}
cor_drop_list <- c('Milk...Excluding.Butter', 'kcal_Alcoholic.Beverages', 
                   'protein_Alcoholic.Beverages', 'kcal_Animal.fats', 'fat_Animal.fats',
                   'protein_Animal.fats', 'protein_Milk...Excluding.Butter', 
                   'protein_Animal.Products', 'kcal_Milk...Excluding.Butter',
                   'kcal_Animal.Products', 'kcal_Aquatic.Products..Other',
                   'fat_Aquatic.Products..Other', 'protein_Aquatic.Products..Other',
                   'protein_Cereals...Excluding.Beer','kcal_Cereals...Excluding.Beer',
                   'kcal_Eggs', 'kcal_Fish..Seafood', 'kcal_Fruits...Excluding.Wine',
                   'kcal_Meat', 'kcal_Miscellaneous', 'kcal_Offals', 'kcal_Oilcrops', 
                   'kcal_Pulses', 'kcal_Spices', 'kcal_Starchy.Roots', 'kcal_Sugar.Crops', 
                   'kcal_Treenuts', 'kcal_Vegetal.Products', 'kcal_Vegetables', 'fat_Eggs',
                   'fat_Fish..Seafood', 'fat_Miscellaneous', 'fat_Offals', 'fat_Oilcrops',
                   'fat_Pulses', 'fat_Starchy.Roots', 'fat_Sugar.Crops', 'fat_Treenuts',
                   'fat_Vegetable.Oils','fat_Vegetables', 'protein_Eggs', 'protein_Fish..Seafood',
                   'protein_Fruits...Excluding.Wine', 'protein_Meat', 'protein_Offals',
                   'protein_Pulses', 'protein_Spices', 'protein_Starchy.Roots', 
                   'protein_Stimulants', 'protein_Sugar.Crops', 'protein_Treenuts',
                   'protein_Vegetables','protein_Miscellaneous','kcal_Vegetable.Oils',
                   'fat_Spices','kcal_Stimulants','fat_Animal.Products',
                   'fat_Alcoholic.Beverages','protein_Vegetal.Products')

combined_numeric <- combined_numeric[ , !names(combined_numeric) %in% cor_drop_list]
```

```{r Data Exploration}
quantvars_summary<-as.data.frame(apply(combined_numeric,2,summary))
quantvars_summary
write.csv(quantvars_summary,"summary_stat.csv", row.names = TRUE)
```
```{r Outliers}

for (i in 1:ncol(combined_numeric)) {
  outlier <- boxplot.stats(combined_numeric[,i])$out
  outlier_index <- which(combined_numeric[,i] %in% c(outlier))
  title <- cbind("outliers for", colnames(combined_numeric)[i])
  print(title, quote=FALSE)
  print(outlier_index)
}

```


```{r Correlation Heatmap}
# Create correlation matrix
cor_matrix <- cor(combined_numeric, method = c("pearson"))

library(reshape2)
melted_cormat <- melt(cor_matrix)

#plotting correlation heat map
library(ggplot2)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
    geom_tile(color = "black") +
  scale_fill_gradientn(colors = hcl.colors(20, "RdYlGn")) +
  coord_fixed() +  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 5), axis.text.y = element_text( size = 5))

```
```{r Histogram: Deaths}
library(ggthemes)
ggplot(data = combined, aes(x=Deaths)) + 
  geom_histogram(colour="black", fill="grey") + geom_vline(aes(xintercept=mean(Deaths)),
            color="red", linetype="dashed", size=0.5) + theme_economist() 
```


```{r Random Forest Regression: Building the Model}
set.seed(1)
library(randomForest)
myforest=randomForest(Deaths~., data=combined, ntree=1000,importance=TRUE, na.action = na.omit)
myforest

#MSE: 0.001295
```


```{r RF Regression: Variable Importance}
x <- list()
y <- list()

for (j in 1:10)
{
  for (i in 1:20)
  {
    impforest_r=randomForest(combined$Deaths~., data=combined, ntree=(500+(j*50)),importance=TRUE, na.action = na.omit)
    
    pred_importance <- importance(impforest_r)
    pred_importance <- pred_importance[order(pred_importance[,1],decreasing=TRUE),]
    x[[i]] <- pred_importance[,1]
    
  }
y[[j]] <- rowMeans(cbind(x[[1]], x[[2]], x[[3]], x[[4]], x[[5]], x[[6]], x[[7]], x[[8]], x[[9]], x[[10]], x[[11]], x[[12]], x[[13]], x[[14]], x[[15]], x[[16]],x[[17]], x[[18]], x[[19]], x[[20]]))
}

importance_df1 <- data.frame(sort(rowMeans(cbind(y[[1]], y[[2]], y[[3]], y[[4]], y[[5]], y[[6]], y[[7]], y[[8]], y[[9]],y[[10]])),decreasing = T))

colnames(importance_df1) <- 'avg_importance'
```

```{r RF Regression Approach 1: Dropping Insignificant Predictors}
### Drop insignificant predictors
rf_drop_list <- c('Undernourished', 'Stimulants', 'fat_Meat', 'Aquatic.Products..Other',
                      'fat_Sugar...Sweeteners','fat_Stimulants', 'Offals', 'Population',
                      'Fruits...Excluding.Wine','Aquatic.Products..Other', 'Vegetables',
                      'Starchy.Roots')

combined_filtered <- combined[ , !names(combined) %in% rf_drop_list]

```

```{r RF Regression Approach 2: Selecting Significant Predictors}
combined_selected <- combined[,c("Deaths","Obesity","Oilcrops","Alcoholic.Beverages",
                                 "Eggs","fat_Vegetal.Products","Animal.Products",
                                 "Vegetal.Products","Animal.fats","protein_Vegetable.Oils")]
```

```{r RF Regression: Revised Importance}
x <- list()
y <- list()
for (j in 1:10)
{
  for (i in 1:20)
  {
    impforest_r=randomForest(combined_selected$Deaths~., data=combined_selected, ntree=(500+(j*50)),importance=TRUE, na.action = na.omit)
    
    pred_importance <- importance(impforest_r)
    pred_importance <- pred_importance[order(pred_importance[,1],decreasing=TRUE),]
    x[[i]] <- pred_importance[,1]
  }
  y[[j]] <- rowMeans(cbind(x[[1]], x[[2]], x[[3]], x[[4]], x[[5]], x[[6]], x[[7]], x[[8]], x[[9]], 
                           x[[10]], x[[11]], x[[12]], x[[13]], x[[14]], x[[15]], x[[16]],
                           x[[17]], x[[18]], x[[19]], x[[20]]))
}

importance_df2 <- data.frame(sort(rowMeans(cbind(y[[1]], y[[2]], y[[3]], y[[4]], y[[5]], y[[6]], y[[7]], y[[8]], y[[9]],y[[10]])),decreasing = T))
```


```{r Random Forest Regression: Testing }
require(caTools)
require(caret)
accuracy_list = rep(NA, 25)
for (i in 1:25)
{
  # Train test split
  sample = sample.split(combined_selected$Deaths, SplitRatio = .7)
  train = subset(combined_selected, sample == TRUE)
  test  = subset(combined_selected, sample == FALSE)
  
  # Build model with training data
  testforest=randomForest(train$Deaths~., data=train, ntree=1000,importance=TRUE, na.action = na.omit)
  
  # Run predictions
  predicted_score = predict(testforest, newdata=test)
  accuracy_list[i] <- mean((predicted_score - test$Deaths)^2)
  
}
mean(accuracy_list)

```


```{r Principal Component Analysis - TO REVIEW}

# Perform PCA
predictors_numeric = predictors[,-25]
pca=prcomp(predictors_numeric, scale=TRUE)
summary(pca)

# Scree plot for PCA - Good for report
plot(pca, type = "l", main = "Scree plot for PCA")

### The first 2 components explain only 32% of variance. Use correlation matrix 
### to shave down first instead.

# Find variable importance through PCA (only numeric)

# PC1
loading_Scores_PC1 <- pca$rotation[,1]       # describe how much each attribute contributes to the principal component
influence_PC1 <- abs(loading_Scores_PC1)     # rank in order of influence, ignore negative/positive influence
influence_PC1ranked <- names(sort(influence_PC1,decreasing = T))    # Output components of PC in order of in

# PC2
loading_Scores_PC2 <- pca$rotation[,2]       # describe how much each attribute contributes to the principal component
influence_PC2 <- abs(loading_Scores_PC2)     # rank in order of influence, ignore negative/positive influence
influence_PC2ranked <- names(sort(influence_PC2,decreasing = T))    # Output components of PC in order of in

# % variance explained
pve=(pca$sdev^2)/sum(pca$sdev^2)
par(mfrow=c(1,2))
plot(pve, ylim=c(0,1))
plot(cumsum(pve), ylim=c(0,1))        # Cumulative sum
par(mfrow=c(1,1))

library(ggfortify)
autoplot(pca, data = pca_data, loadings = TRUE, col="grey", loadings.label = TRUE )
```




```{r Target Variable Transformation}
#adding Deaths as a categorical variable 
library(Hmisc)
combined$Deaths_cat <- cut2(combined$Deaths, g=3, labels = c("Low Risk", "Medium Risk", "High Risk"))
combined$Deaths_cat <- as.character(combined$Deaths_cat)

table(combined$Deaths_cat)

combined$Deaths_cat[combined$Deaths_cat == "[0.00000,0.00438)"] <- "Low Risk"
combined$Deaths_cat[combined$Deaths_cat == "[0.00438,0.05095)"] <- "Medium Risk"
combined$Deaths_cat[combined$Deaths_cat == "[0.05095,0.18543]"] <- "High Risk"

combined$Deaths_cat <- as.factor(combined$Deaths_cat)

# Drop deaths column
drop_list <- c('Deaths')
combined <- combined[ , !names(combined) %in% drop_list]
```

```{r Random Forest Classification: Variable Importance}
x <- list()
y <- list()
for (j in 1:10)
{
  for (i in 1:20)
  {
    impforest=randomForest(combinedcat$Deaths_cat~., data=combinedcat, ntree=(500+(j*50)),importance=TRUE, na.action = na.omit)
    
    pred_importance <- importance(impforest)
    pred_importance <- pred_importance[order(pred_importance[,1],decreasing=TRUE),]
    x[[i]] <- pred_importance[,4]
  }
  y[[j]] <- rowMeans(cbind(x[[1]], x[[2]], x[[3]], x[[4]], x[[5]], x[[6]], x[[7]], x[[8]], x[[9]], 
                           x[[10]], x[[11]], x[[12]], x[[13]], x[[14]], x[[15]], x[[16]],
                           x[[17]], x[[18]], x[[19]], x[[20]]))
}

class_importance_df1 <- data.frame(sort(rowMeans(cbind(y[[1]], y[[2]], y[[3]], y[[4]], y[[5]], y[[6]], y[[7]], y[[8]], y[[9]],y[[10]])),decreasing = T))
  
```


```{r RF Classification Approach 1: Dropping Insignificant Predictors}
### Drop insignificant predictors
rf_drop_list_cat <- c('Undernourished', 'Stimulants', 'fat_Meat', 'Aquatic.Products..Other',
                  'fat_Sugar...Sweeteners','fat_Stimulants', 'Offals', 'Population',
                  'Fruits...Excluding.Wine','Aquatic.Products..Other', 'Vegetables',
                  'Starchy.Roots')

combinedcat_filtered <- combinedcat[ , !names(combinedcat) %in% rf_drop_list_cat]

```

```{r RF Classification Approach 2: Selecting Significant Predictors}
combinedcat_selected <- combinedcat[,c("Deaths_cat","Obesity", "Animal.Products", "Animal.fats",
                                       "Vegetal.Products","Eggs","Oilcrops","Fish..Seafood",
                                       "fat_Milk...Excluding.Butter","kcal_Sugar...Sweeteners",
                                       "Alcoholic.Beverages","Treenuts","fat_Vegetal.Products",
                                       "fat_Cereals...Excluding.Beer")]
```


```{r RF Revised Variable Importance}
x <- list()
y <- list()
for (j in 1:10)
{
  for (i in 1:20)
  {
    impforest=randomForest(combinedcat_selected$Deaths_cat~., data=combinedcat_selected, ntree=(500+(j*50)),importance=TRUE, na.action = na.omit)
    
    pred_importance <- importance(impforest)
    pred_importance <- pred_importance[order(pred_importance[,1],decreasing=TRUE),]
    x[[i]] <- pred_importance[,4]
  }
  y[[j]] <- rowMeans(cbind(x[[1]], x[[2]], x[[3]], x[[4]], x[[5]], x[[6]], x[[7]], x[[8]], x[[9]], 
                           x[[10]], x[[11]], x[[12]], x[[13]], x[[14]], x[[15]], x[[16]],
                           x[[17]], x[[18]], x[[19]], x[[20]]))
}

class_importance_df2 <- sort(rowMeans(cbind(y[[1]], y[[2]], y[[3]], y[[4]], y[[5]], y[[6]], y[[7]], y[[8]], y[[9]], y[[10]])),decreasing = T)
```


```{r Random Forest Classification: Building and Testing the model}
require(caTools)
require(caret)
accuracy_list = rep(NA, 25)
for (i in 1:25)
{
  # Train test split
  sample = sample.split(combinedcat_selected$Deaths_cat, SplitRatio = .7)
  train = subset(combinedcat_selected, sample == TRUE)
  test  = subset(combinedcat_selected, sample == FALSE)
  
  # Build model with training data
  testforestcat=randomForest(train$Deaths_cat~., data=train, ntree=1000,importance=TRUE, na.action = na.omit)
  
  # Run predictions
  predicted_score = predict(testforestcat, newdata=test)
  
  #confusionMatrix(predicted_score, test$Deaths_cat)    # For classification forest
  #cm$overall['Accuracy']
  accuracy_list[i] <- confusionMatrix(predicted_score, test$Deaths_cat)$overall['Accuracy']
}
mean(accuracy_list)
```

```{r LDA/QDA: Probability Density Functions}
#calculating prior probabilities
prior_prob<-data.frame(table(combined$Deaths_cat)/nrow(combined))
colnames(prior_prob) <- c("category", "probability")

#performing LDA on the following two predictors: obesity, Alcoholic.Beverages
#plot histogram
hist1 = ggplot(combined, aes(x=Obesity)) + geom_histogram(bins = 40) + facet_grid(combined$Deaths_cat)
hist1

hist2 = ggplot(combined, aes(x=Alcoholic.Beverages)) + geom_histogram(bins = 40) + facet_grid(combined$Deaths_cat)
hist2
```

```{r Basic LDA Model: Obesity and Alcoholic.Beverages}
#install.packages("MASS")
#install.packages("klaR")
library(MASS)
library(klaR)

lda = lda(combined$Deaths_cat~combined$Obesity+combined$Alcoholic.Beverages)
lda

partimat = partimat(combined$Deaths_cat~combined$Obesity+combined$Alcoholic.Beverages)
partimat
```

```{r QDA Model: : Obesity and Alcoholic.Beverages}
qda = qda(combined$Deaths_cat~combined$Obesity+combined$Alcoholic.Beverages)
qda

partimat2 = partimat(combined$Deaths_cat~combined$Obesity+combined$Alcoholic.Beverages, method="qda")
partimat2
```

```{r TO DO: Multiple LDA Model}
lda = lda(Deaths~., data=combined)
lda
```

```{r K-Means Clustering}
#can countries be clustered into groups
#do these clusters have specific characters 
# how does it compare to income group clusters (does income group of a category impact death rate)

head(full_data)
```

```{r Visualizing Data}
#can improve this graph with labelling country color -- income group
library(ggplot2)
library(ggthemes)
plot=ggplot(full_data,aes(y=Deaths, x=Obesity, label =Country))
plot+geom_point(alpha = 0.6) +  theme_economist() 
#+geom_text(aes(label=full_data$Country))

```



```{r K- Means Approach 1: Selecting Significant Predictors}
combined_selected <- combined[,c("Deaths","Obesity","Oilcrops","Alcoholic.Beverages",
                                 "Eggs","fat_Vegetal.Products","Animal.Products",
                                 "Vegetal.Products","Animal.fats","protein_Vegetable.Oils")]

combined_kmeans1 <- kmeans_processing(combined_selected)
```

```{r K- Means Approach 2: Dropping Irrelevant Predictors}
rf_drop_list <- c('Undernourished', 'Stimulants', 'fat_Meat', 'Aquatic.Products..Other',
                      'fat_Sugar...Sweeteners','fat_Stimulants', 'Offals', 'Population',
                      'Fruits...Excluding.Wine','Aquatic.Products..Other', 'Vegetables',
                      'Starchy.Roots')

combined_filtered <- combined[ , !names(combined) %in% rf_drop_list]

combined_kmeans2 <- kmeans_processing(combined_filtered)
```

```{r}

```


```{r K-Means: Data Preprocessing}
rownames(combined) <- full_data$Country
head(combined)

#check for nulls 
nulls <- colSums(is.na(combined))
na_count <- data.frame( nulls)

library(dplyr)
na_count %>% filter(nulls > 0)

#defining a function that creates dummies and scales data
kmeans_processing <- function(combined)
{#dummifying undernourished & dropping the categorical variable
library(fastDummies)
if ('Undernourished' %in% colnames(combined)){
combined <- dummy_cols(combined, select_columns = 'Undernourished', remove_first_dummy = TRUE, remove_selected_columns = TRUE)}

#dummifying deaths_cat & dropping the categorical variable
combined <- dummy_cols(combined, select_columns = 'Deaths_cat', remove_first_dummy = TRUE, remove_selected_columns = TRUE)

#standardizing values 
scaled.data <- scale(combined)

return(scaled.data)}

#check if the column type is numeric 
col_type <- lapply(combined, class)

# check that we get mean of 0 and sd of 1
#colMeans(scaled.data)  # faster version of apply(scaled.dat, 2, mean)
#apply(scaled.data, 2, sd)

#TO DO: reducing dimensions (cherry picking relevant predictors)
```

```{r K-Means: Distance Matrix Visualization}
library(factoextra)
distance <- get_dist(scaled.data)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"), lab_size = 3)
```

```{r K-Means: Optimal Number of Clusters}
set.seed(123)

# function to compute total within-cluster sum of square 
within_val <- function(k) {
  kmeans(scaled.data, k, nstart = 10)$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k_vals <- seq(1, 15)

# extract wss for 2-15 clusters
wss_values <- c()

for (i in k_vals){
  new_val <- within_val(i)
  wss_values <- c(wss_values, new_val)
}

#wss_values <- data.frame(k_vals, wss_values)
plot(k_vals, wss_values,
       type="b", pch = 19, frame = FALSE, 
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")

#let's say the optimal number of clusters: 2 
```

```{r K-Means: Modelling - 2 clusters }
k2 <- kmeans(scaled.data, centers = 2, nstart = 25)
str(k2)
scaled.data$cluster=as.factor(k2$cluster) 
```

```{r}
plot=ggplot(full_data,aes(y=Deaths, x=Obesity, label =Country))
plot+geom_point(alpha = 0.6) +  theme_economist()
plot+geom_point(aes(colour=scaled.data$cluster))
```


```{r K-Means: Data Preprocessing}
rownames(combined) <- full_data$Country
head(combined)

#check for nulls 
nulls <- colSums(is.na(combined))
na_count <- data.frame( nulls)

library(dplyr)
na_count %>% filter(nulls > 0)

#create dummies 
#question: should dummies be standardized for kmeans

#dummifying undernourished & dropping the categorical variable
table(combined$Undernourished)
library(fastDummies)
combined <- dummy_cols(combined, select_columns = 'Undernourished', remove_first_dummy = TRUE, remove_selected_columns = TRUE)

#dummifying deaths_cat & dropping the categorical variable
table(combined$Deaths_cat)

combined <- dummy_cols(combined, select_columns = 'Deaths_cat', remove_first_dummy = TRUE, remove_selected_columns = TRUE)

#check if the column type is numeric 
col_type <- lapply(combined, class)

#standardizing values 
scaled.data <- scale(combined)

# check that we get mean of 0 and sd of 1
#colMeans(scaled.data)  # faster version of apply(scaled.dat, 2, mean)
#apply(scaled.data, 2, sd)

#TO DO: reducing dimensions (cherry picking relevant predictors)
```


```{r K-Means: Modelling - 6 clusters }
k6 <- kmeans(scaled.data, centers = 6, nstart = 25)
str(k6)
scaled.data$cluster2=as.factor(k6$cluster) 
```

```{r}
plot=ggplot(full_data,aes(y=Deaths, x=Obesity, label =Country))
plot+geom_point(alpha = 0.6) +  theme_economist()
plot+geom_point(aes(colour=scaled.data$cluster2))
```
